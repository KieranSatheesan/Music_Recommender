{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e42cf4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# If gensim isn't installed yet: in a separate cell run:\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# %pip install gensim\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Make plots a bit nicer\u001b[39;00m\n\u001b[32m     18\u001b[39m plt.style.use(\u001b[33m\"\u001b[39m\u001b[33mggplot\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Imports & paths\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# If gensim isn't installed yet: in a separate cell run:\n",
    "# %pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Make plots a bit nicer\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "INTERACTIONS_PATH = DATA_PROCESSED / \"interactions.parquet\"\n",
    "TRACK_META_PATH = DATA_PROCESSED / \"track_metadata.csv\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"INTERACTIONS_PATH:\", INTERACTIONS_PATH)\n",
    "print(\"TRACK_META_PATH:\", TRACK_META_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aadce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load interactions & metadata\n",
    "\n",
    "inter = pd.read_parquet(INTERACTIONS_PATH)\n",
    "track_meta = pd.read_csv(TRACK_META_PATH)\n",
    "\n",
    "print(\"Raw interactions shape:\", inter.shape)\n",
    "display(inter.head())\n",
    "\n",
    "print(\"\\ntrack_metadata.csv shape:\", track_meta.shape)\n",
    "display(track_meta.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17705643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Basic sanity checks\n",
    "\n",
    "n_playlists = inter[\"pid\"].nunique()\n",
    "n_tracks = inter[\"track_id\"].nunique()\n",
    "\n",
    "print(f\"Num playlists in interactions: {n_playlists:,}\")\n",
    "print(f\"Num unique tracks in interactions: {n_tracks:,}\")\n",
    "\n",
    "# Sort interactions inside playlists by pos (or duration if you prefer)\n",
    "inter = inter.sort_values([\"pid\", \"pos\"]).reset_index(drop=True)\n",
    "\n",
    "# Meta sanity\n",
    "print(\"\\nMetadata coverage of interaction tracks:\")\n",
    "covered = inter[\"track_id\"].isin(track_meta[\"track_id\"]).mean() * 100\n",
    "print(f\"  % of interaction track_ids that have metadata: {covered:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Build playlist sentences (track sequences)\n",
    "\n",
    "MIN_LEN = 5  # drop very tiny playlists for training stability\n",
    "\n",
    "playlist_groups = inter.groupby(\"pid\")[\"track_id\"].apply(list)\n",
    "\n",
    "print(\"Total playlists:\", len(playlist_groups))\n",
    "\n",
    "playlist_sentences = [tracks for tracks in playlist_groups if len(tracks) >= MIN_LEN]\n",
    "\n",
    "print(f\"Playlists kept (len >= {MIN_LEN}): {len(playlist_sentences)}\")\n",
    "print(\"Example playlist sentence (first 15 tracks):\")\n",
    "print(playlist_sentences[0][:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782416e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Train Item2Vec model\n",
    "\n",
    "# Hyperparameters (tune later if you want)\n",
    "EMBED_DIM = 64        # size of embedding vector\n",
    "WINDOW = 20           # how many neighbours left/right to consider\n",
    "MIN_COUNT = 2         # ignore tracks that appear < 2 times\n",
    "SG = 1                # 1 = skip-gram, 0 = CBOW\n",
    "NEGATIVE = 10         # negative samples\n",
    "EPOCHS = 10\n",
    "\n",
    "print(\"Training Word2Vec (Item2Vec) model...\")\n",
    "print(f\"  sentences: {len(playlist_sentences):,}\")\n",
    "print(f\"  embedding dim: {EMBED_DIM}, window: {WINDOW}, min_count: {MIN_COUNT}\")\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=playlist_sentences,\n",
    "    vector_size=EMBED_DIM,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=4,\n",
    "    sg=SG,\n",
    "    negative=NEGATIVE,\n",
    "    epochs=EPOCHS,\n",
    ")\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# Save model so you can reload later without retraining\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "ITEM2VEC_PATH = MODELS_DIR / \"item2vec_word2vec.model\"\n",
    "model.save(str(ITEM2VEC_PATH))\n",
    "print(\"Saved model to:\", ITEM2VEC_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Inspect vocabulary & coverage\n",
    "\n",
    "vocab_tracks = set(model.wv.index_to_key)\n",
    "print(f\"Tracks in Item2Vec vocab (after min_count): {len(vocab_tracks):,}\")\n",
    "\n",
    "inter_tracks = set(inter[\"track_id\"].unique())\n",
    "meta_tracks = set(track_meta[\"track_id\"].unique())\n",
    "\n",
    "print(f\"Tracks in interactions: {len(inter_tracks):,}\")\n",
    "print(f\"Tracks in track_metadata: {len(meta_tracks):,}\")\n",
    "\n",
    "print(\"\\nCoverage:\")\n",
    "print(f\"% of interaction tracks in vocab: {len(inter_tracks & vocab_tracks) / len(inter_tracks) * 100:.2f}%\")\n",
    "print(f\"% of vocab tracks with metadata: {len(vocab_tracks & meta_tracks) / len(vocab_tracks) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8999d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Helper: find track IDs by name substring\n",
    "\n",
    "# Keep a slim view of metadata for display\n",
    "meta_simple = (\n",
    "    track_meta[[\"track_id\", \"track_name\", \"artist_name\", \"album_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"track_id\")\n",
    ")\n",
    "\n",
    "def find_track_ids_by_name(query, top_n=20):\n",
    "    \"\"\"\n",
    "    Simple case-insensitive substring search over track_name and artist_name.\n",
    "    Returns a small DataFrame of candidate tracks.\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "    mask = (\n",
    "        track_meta[\"track_name\"].str.lower().str.contains(q, na=False)\n",
    "        | track_meta[\"artist_name\"].str.lower().str.contains(q, na=False)\n",
    "    )\n",
    "    candidates = (\n",
    "        track_meta.loc[mask, [\"track_id\", \"track_name\", \"artist_name\", \"album_name\"]]\n",
    "        .drop_duplicates()\n",
    "        .head(top_n)\n",
    "    )\n",
    "    return candidates\n",
    "\n",
    "# Example:\n",
    "display(find_track_ids_by_name(\"ocean eyes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c04401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Recommend similar songs via Item2Vec\n",
    "\n",
    "def describe_tracks(track_ids):\n",
    "    \"\"\"\n",
    "    Given a list/array of track_ids, return a table with names.\n",
    "    \"\"\"\n",
    "    ids = list(track_ids)\n",
    "    df = meta_simple.loc[meta_simple.index.intersection(ids)].copy()\n",
    "    df = df.reset_index()\n",
    "    # Keep the same order as ids\n",
    "    df = df.set_index(\"track_id\").loc[ids].reset_index()\n",
    "    return df\n",
    "\n",
    "def recommend_by_name_item2vec(query, candidate_index=0, top_k=15):\n",
    "    \"\"\"\n",
    "    1) Search for tracks whose name/artist match `query`.\n",
    "    2) Let you pick one by index (`candidate_index` from the search table).\n",
    "    3) Use Item2Vec to find the most similar tracks.\n",
    "    4) Return a DataFrame of recommended tracks + similarity.\n",
    "    \"\"\"\n",
    "    candidates = find_track_ids_by_name(query, top_n=50)\n",
    "    if candidates.empty:\n",
    "        print(f\"No matches found for query '{query}'\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"Search results:\")\n",
    "    display(candidates.reset_index(drop=True))\n",
    "\n",
    "    if candidate_index >= len(candidates):\n",
    "        raise ValueError(f\"candidate_index {candidate_index} out of range (num results={len(candidates)})\")\n",
    "    \n",
    "    seed_row = candidates.iloc[candidate_index]\n",
    "    seed_tid = seed_row[\"track_id\"]\n",
    "    \n",
    "    print(\"\\nChosen seed track:\")\n",
    "    display(seed_row.to_frame().T)\n",
    "\n",
    "    if seed_tid not in model.wv:\n",
    "        print(\"\\n[Item2Vec] Seed track is not in vocab (too rare for MIN_COUNT). \"\n",
    "              \"Try another candidate or lower MIN_COUNT and retrain.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get most similar tracks in embedding space\n",
    "    sims = model.wv.most_similar(seed_tid, topn=top_k)\n",
    "    rec_ids = [tid for tid, score in sims]\n",
    "    rec_scores = [score for tid, score in sims]\n",
    "\n",
    "    rec_df = describe_tracks(rec_ids)\n",
    "    rec_df[\"item2vec_similarity\"] = rec_scores\n",
    "    return rec_df\n",
    "\n",
    "# Example usage (run and experiment):\n",
    "# recs = recommend_by_name_item2vec(\"ocean eyes\", candidate_index=2, top_k=15)\n",
    "# display(recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Prepare a sample of track embeddings for visualisation\n",
    "\n",
    "# Take intersection of model vocab and metadata so we can colour by features\n",
    "vocab_ids = set(model.wv.index_to_key)\n",
    "meta_ids = set(track_meta[\"track_id\"].unique())\n",
    "vis_ids = list(vocab_ids & meta_ids)\n",
    "\n",
    "print(f\"Tracks usable for visualisation (in vocab & metadata): {len(vis_ids):,}\")\n",
    "\n",
    "# Sample subset to keep t-SNE manageable\n",
    "N_TSNE = 3000\n",
    "if len(vis_ids) > N_TSNE:\n",
    "    vis_ids_sample = random.sample(vis_ids, N_TSNE)\n",
    "else:\n",
    "    vis_ids_sample = vis_ids\n",
    "\n",
    "print(f\"Using {len(vis_ids_sample)} tracks for t-SNE visualisation.\")\n",
    "\n",
    "# Build embedding matrix\n",
    "emb_matrix = np.vstack([model.wv[tid] for tid in vis_ids_sample])\n",
    "\n",
    "# Build a small metadata frame for the sampled tracks\n",
    "vis_meta = (\n",
    "    track_meta[track_meta[\"track_id\"].isin(vis_ids_sample)]\n",
    "    .drop_duplicates(\"track_id\")\n",
    "    .set_index(\"track_id\")\n",
    "    .loc[vis_ids_sample]  # preserve order\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"emb_matrix shape:\", emb_matrix.shape)\n",
    "display(vis_meta.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e24a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Run t-SNE (this can take a bit of time)\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    n_iter=1000,\n",
    "    metric=\"euclidean\",\n",
    "    init=\"random\",\n",
    "    random_state=42,\n",
    ")\n",
    "X_tsne = tsne.fit_transform(emb_matrix)\n",
    "print(\"X_tsne shape:\", X_tsne.shape)\n",
    "\n",
    "vis_meta[\"tsne_x\"] = X_tsne[:, 0]\n",
    "vis_meta[\"tsne_y\"] = X_tsne[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ff075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 — Plot t-SNE, coloured by a feature (e.g. danceability)\n",
    "\n",
    "# Merge in audio features from combined_features (if you want richer colouring).\n",
    "# If you *don't* have combined_features handy in this notebook, skip the merge.\n",
    "# Here I'll assume you do:\n",
    "\n",
    "COMBINED_FEATURES_PATH = DATA_PROCESSED / \"combined_features.csv\"\n",
    "\n",
    "cf = pd.read_csv(COMBINED_FEATURES_PATH)\n",
    "cf_small = cf[[\"track_id\", \"danceability\", \"energy\", \"valence\", \"tempo\"]].drop_duplicates(\"track_id\")\n",
    "\n",
    "vis = vis_meta.merge(cf_small, on=\"track_id\", how=\"left\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sc = ax.scatter(\n",
    "    vis[\"tsne_x\"],\n",
    "    vis[\"tsne_y\"],\n",
    "    c=vis[\"danceability\"],\n",
    "    s=10,\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.colorbar(sc, label=\"danceability\")\n",
    "ax.set_title(\"Item2Vec track embeddings (t-SNE projection) coloured by danceability\")\n",
    "ax.set_xlabel(\"t-SNE dim 1\")\n",
    "ax.set_ylabel(\"t-SNE dim 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Another view: coloured by energy\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sc = ax.scatter(\n",
    "    vis[\"tsne_x\"],\n",
    "    vis[\"tsne_y\"],\n",
    "    c=vis[\"energy\"],\n",
    "    s=10,\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.colorbar(sc, label=\"energy\")\n",
    "ax.set_title(\"Item2Vec track embeddings (t-SNE projection) coloured by energy\")\n",
    "ax.set_xlabel(\"t-SNE dim 1\")\n",
    "ax.set_ylabel(\"t-SNE dim 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Helper to plot a local neighbourhood for a given seed track\n",
    "\n",
    "def plot_item2vec_neighbourhood(seed_track_id, top_k=15):\n",
    "    \"\"\"\n",
    "    Plot seed track + its Item2Vec neighbours on the t-SNE scatter.\n",
    "    \"\"\"\n",
    "    if seed_track_id not in model.wv:\n",
    "        print(\"Seed track not in Item2Vec vocab.\")\n",
    "        return\n",
    "    \n",
    "    # nearest neighbours in embedding space\n",
    "    sims = model.wv.most_similar(seed_track_id, topn=top_k)\n",
    "    neighbour_ids = [tid for tid, _ in sims]\n",
    "    neighbour_scores = [score for _, score in sims]\n",
    "    \n",
    "    # Ensure these ids exist in our vis DF\n",
    "    ids_to_plot = [seed_track_id] + neighbour_ids\n",
    "    mask = vis[\"track_id\"].isin(ids_to_plot)\n",
    "    sub = vis[mask].copy()\n",
    "    \n",
    "    # Label seed vs neighbours\n",
    "    sub[\"type\"] = np.where(sub[\"track_id\"] == seed_track_id, \"seed\", \"neighbour\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    \n",
    "    # Plot all background points faintly\n",
    "    ax.scatter(vis[\"tsne_x\"], vis[\"tsne_y\"], s=5, alpha=0.1, label=\"others\")\n",
    "    \n",
    "    # Plot neighbours\n",
    "    nb = sub[sub[\"type\"] == \"neighbour\"]\n",
    "    ax.scatter(nb[\"tsne_x\"], nb[\"tsne_y\"], s=40, alpha=0.8, label=\"neighbours\")\n",
    "    \n",
    "    # Plot seed\n",
    "    sd = sub[sub[\"type\"] == \"seed\"]\n",
    "    ax.scatter(sd[\"tsne_x\"], sd[\"tsne_y\"], s=120, marker=\"*\", label=\"seed\", edgecolor=\"black\")\n",
    "    \n",
    "    for _, row in sub.iterrows():\n",
    "        ax.text(\n",
    "            row[\"tsne_x\"],\n",
    "            row[\"tsne_y\"],\n",
    "            row[\"track_name\"][:15],\n",
    "            fontsize=7,\n",
    "            alpha=0.9,\n",
    "        )\n",
    "    \n",
    "    ax.set_title(\"Local Item2Vec neighbourhood (t-SNE space)\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# 1) Find a track-id via name\n",
    "# candidates = find_track_ids_by_name(\"ocean eyes\")\n",
    "# display(candidates)\n",
    "\n",
    "# 2) Pick the one you want:\n",
    "# seed_tid = candidates.iloc[2][\"track_id\"]\n",
    "# plot_item2vec_neighbourhood(seed_tid, top_k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Rough evaluation: can Item2Vec recover held-out songs?\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "pids = inter[\"pid\"].unique()\n",
    "SAMPLE_PLAYLISTS = 500\n",
    "TOP_K = 20\n",
    "\n",
    "sample_pids = rng.choice(pids, size=min(SAMPLE_PLAYLISTS, len(pids)), replace=False)\n",
    "\n",
    "hits = 0\n",
    "total = 0\n",
    "\n",
    "for pid in sample_pids:\n",
    "    seq = playlist_groups.loc[pid]\n",
    "    if len(seq) < 6:\n",
    "        continue\n",
    "\n",
    "    # Very simple: treat last track as \"held out\"\n",
    "    held_out = seq[-1]\n",
    "    context_tracks = [tid for tid in seq[:-1] if tid in model.wv]\n",
    "\n",
    "    if not context_tracks or held_out not in model.wv:\n",
    "        continue\n",
    "\n",
    "    # Combine neighbours from some context tracks\n",
    "    candidate_scores = {}\n",
    "\n",
    "    for tid in context_tracks[-5:]:  # last few tracks in playlist\n",
    "        try:\n",
    "            sims = model.wv.most_similar(tid, topn=TOP_K)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for nb_tid, score in sims:\n",
    "            candidate_scores[nb_tid] = max(candidate_scores.get(nb_tid, 0.0), score)\n",
    "\n",
    "    # Top-K candidates overall\n",
    "    if not candidate_scores:\n",
    "        continue\n",
    "\n",
    "    ranked = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_ids = [tid for tid, _ in ranked[:TOP_K]]\n",
    "\n",
    "    total += 1\n",
    "    if held_out in top_ids:\n",
    "        hits += 1\n",
    "\n",
    "if total > 0:\n",
    "    hit_rate = hits / total\n",
    "    print(f\"Evaluated on {total} playlists.\")\n",
    "    print(f\"HitRate@{TOP_K}: {hit_rate:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough playlists passed the filtering for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = recommend_by_name_item2vec(\"hotel room service\", candidate_index=1, top_k=15)\n",
    "display(recs)\n",
    "\n",
    "recs = recommend_by_name_item2vec(\"ocean eyes\", candidate_index=2, top_k=15)\n",
    "display(recs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
