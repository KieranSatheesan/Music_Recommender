{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41203a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kiera\\Music_Recommender\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\Kiera\\Music_Recommender\n",
      "DATA_PROCESSED: C:\\Users\\Kiera\\Music_Recommender\\data\\processed\n",
      "MODELS_DIR: C:\\Users\\Kiera\\Music_Recommender\\models\n"
     ]
    }
   ],
   "source": [
    "# 01 - Imports & paths\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ALS (implicit feedback)\n",
    "import implicit\n",
    "\n",
    "# Item2Vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# Sparse matrices\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"MODELS_DIR:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76407f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (4589288, 17)\n",
      "Track metadata shape: (679889, 7)\n",
      "Interactions shape: (6685101, 4)\n",
      "Tracks present in ALL (features+meta+interactions): 514672\n",
      "Filtered shapes:\n",
      "  features: (514672, 17)\n",
      "  meta    : (514672, 7)\n",
      "  inter   : (6175813, 4)\n"
     ]
    }
   ],
   "source": [
    "# 02 - Load combined features, metadata, and interactions; intersect on track_id\n",
    "\n",
    "FEATURES_PATH      = DATA_PROCESSED / \"combined_features.csv\"\n",
    "INTERACTIONS_PATH  = DATA_PROCESSED / \"interactions.parquet\"\n",
    "TRACK_META_PATH    = DATA_PROCESSED / \"track_metadata.csv\"\n",
    "\n",
    "# --- Load combined features ---\n",
    "feat_df = pd.read_csv(FEATURES_PATH)\n",
    "print(\"Features shape:\", feat_df.shape)\n",
    "\n",
    "# --- Load track metadata (for names, URIs, etc.) ---\n",
    "meta_df = pd.read_csv(TRACK_META_PATH)\n",
    "print(\"Track metadata shape:\", meta_df.shape)\n",
    "\n",
    "# --- Load interactions (pid, track_id, pos, duration_ms, ...) ---\n",
    "inter_df = pd.read_parquet(INTERACTIONS_PATH)\n",
    "print(\"Interactions shape:\", inter_df.shape)\n",
    "\n",
    "# Ensure track_id is string everywhere\n",
    "for df_ in (feat_df, meta_df, inter_df):\n",
    "    df_[\"track_id\"] = df_[\"track_id\"].astype(str)\n",
    "\n",
    "# Keep only tracks that exist in all 3 (features + metadata + interactions)\n",
    "track_ids_all = (\n",
    "    set(feat_df[\"track_id\"])\n",
    "    & set(meta_df[\"track_id\"])\n",
    "    & set(inter_df[\"track_id\"])\n",
    ")\n",
    "\n",
    "print(\"Tracks present in ALL (features+meta+interactions):\", len(track_ids_all))\n",
    "\n",
    "feat_df = feat_df[feat_df[\"track_id\"].isin(track_ids_all)].reset_index(drop=True)\n",
    "meta_df = meta_df[meta_df[\"track_id\"].isin(track_ids_all)].reset_index(drop=True)\n",
    "inter_df = inter_df[inter_df[\"track_id\"].isin(track_ids_all)].reset_index(drop=True)\n",
    "\n",
    "print(\"Filtered shapes:\")\n",
    "print(\"  features:\", feat_df.shape)\n",
    "print(\"  meta    :\", meta_df.shape)\n",
    "print(\"  inter   :\", inter_df.shape)\n",
    "\n",
    "# Simple metadata index for lookups (global)\n",
    "meta_simple = (\n",
    "    meta_df[[\"track_id\", \"track_name\", \"artist_name\", \"album_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"track_id\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc97c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns used (15):\n",
      "['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'year', 'time_signature', 'popularity']\n",
      "X_norm_all shape: (514672, 15)\n",
      "Mean row norm (all): 1.0\n",
      "Built cosine KNN index on ALL intersection.\n"
     ]
    }
   ],
   "source": [
    "# 03 - Build numeric feature matrix + cosine KNN index (ALL intersection)\n",
    "\n",
    "# Choose numeric feature columns (everything except ids / non-numerics)\n",
    "non_feature_cols = {\"track_id\", \"explicit\"}  # explicit often messy / non-numeric\n",
    "feature_cols = [\n",
    "    c for c in feat_df.columns\n",
    "    if c not in non_feature_cols and np.issubdtype(feat_df[c].dtype, np.number)\n",
    "]\n",
    "\n",
    "print(\"Feature columns used ({}):\".format(len(feature_cols)))\n",
    "print(feature_cols)\n",
    "\n",
    "# Impute NaNs with column means\n",
    "feat_mat = feat_df[feature_cols].copy()\n",
    "feat_mat = feat_mat.fillna(feat_mat.mean())\n",
    "\n",
    "# Standardise then L2-normalise rows\n",
    "scaler_all = StandardScaler()\n",
    "X_scaled_all = scaler_all.fit_transform(feat_mat.values)\n",
    "\n",
    "row_norms_all = np.linalg.norm(X_scaled_all, axis=1, keepdims=True)\n",
    "row_norms_all[row_norms_all == 0.0] = 1.0\n",
    "X_norm_all = X_scaled_all / row_norms_all\n",
    "\n",
    "print(\"X_norm_all shape:\", X_norm_all.shape)\n",
    "print(\"Mean row norm (all):\", float(np.linalg.norm(X_norm_all, axis=1).mean()))\n",
    "\n",
    "# Track index mapping for feature space\n",
    "track_ids_all_arr = feat_df[\"track_id\"].values\n",
    "tid_to_feat_idx_all = {tid: i for i, tid in enumerate(track_ids_all_arr)}\n",
    "\n",
    "# Cosine KNN index (exact NN with brute-force cosine)\n",
    "knn_cosine_all = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "knn_cosine_all.fit(X_norm_all)\n",
    "\n",
    "print(\"Built cosine KNN index on ALL intersection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2750f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Item2Vec model from: C:\\Users\\Kiera\\Music_Recommender\\models\\item2vec_word2vec.model\n",
      "Item2Vec vocab size: 306873\n"
     ]
    }
   ],
   "source": [
    "# 04 - Load Item2Vec (Word2Vec) model\n",
    "\n",
    "item2vec_path = MODELS_DIR / \"item2vec_word2vec.model\"\n",
    "\n",
    "if not item2vec_path.exists():\n",
    "    raise FileNotFoundError(f\"Item2Vec model not found at: {item2vec_path}\")\n",
    "\n",
    "print(\"Loading Item2Vec model from:\", item2vec_path)\n",
    "\n",
    "# Load full Word2Vec model\n",
    "item2vec_model = Word2Vec.load(str(item2vec_path))\n",
    "\n",
    "# We'll use the keyed vectors as our \"track embedding\" lookup\n",
    "item2vec_kv = item2vec_model.wv\n",
    "\n",
    "print(\"Item2Vec vocab size:\", len(item2vec_kv.key_to_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8203246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track universe size (features+meta+inter+item2vec): 244370\n",
      "Filtered shapes (HYBRID universe):\n",
      "  features: (244370, 17)\n",
      "  meta    : (244370, 7)\n",
      "  inter   : (5905511, 4)\n",
      "Unified cosine index built on HYBRID universe.\n"
     ]
    }
   ],
   "source": [
    "# 05 - Hybrid track universe: tracks with features + meta + interactions + item2vec\n",
    "\n",
    "track_universe = (\n",
    "    set(feat_df[\"track_id\"])\n",
    "    & set(meta_df[\"track_id\"])\n",
    "    & set(inter_df[\"track_id\"])\n",
    "    & set(item2vec_kv.key_to_index.keys())\n",
    ")\n",
    "\n",
    "print(\"Track universe size (features+meta+inter+item2vec):\", len(track_universe))\n",
    "\n",
    "# Filter all three to this smaller, fully-covered universe\n",
    "feat_df_u = feat_df[feat_df[\"track_id\"].isin(track_universe)].reset_index(drop=True)\n",
    "meta_df_u = meta_df[meta_df[\"track_id\"].isin(track_universe)].reset_index(drop=True)\n",
    "inter_df_u = inter_df[inter_df[\"track_id\"].isin(track_universe)].reset_index(drop=True)\n",
    "\n",
    "print(\"Filtered shapes (HYBRID universe):\")\n",
    "print(\"  features:\", feat_df_u.shape)\n",
    "print(\"  meta    :\", meta_df_u.shape)\n",
    "print(\"  inter   :\", inter_df_u.shape)\n",
    "\n",
    "# Rebuild feature matrix + scaling + normalisation on this universe\n",
    "feat_mat_u = feat_df_u[feature_cols].copy()\n",
    "feat_mat_u = feat_mat_u.fillna(feat_mat_u.mean())\n",
    "\n",
    "scaler_u = StandardScaler()\n",
    "X_scaled_u = scaler_u.fit_transform(feat_mat_u.values)\n",
    "\n",
    "row_norms_u = np.linalg.norm(X_scaled_u, axis=1, keepdims=True)\n",
    "row_norms_u[row_norms_u == 0.0] = 1.0\n",
    "X_norm_u = X_scaled_u / row_norms_u\n",
    "\n",
    "track_ids_u = feat_df_u[\"track_id\"].values\n",
    "tid_to_feat_idx_u = {tid: i for i, tid in enumerate(track_ids_u)}\n",
    "\n",
    "knn_cosine_u = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "knn_cosine_u.fit(X_norm_u)\n",
    "\n",
    "print(\"Unified cosine index built on HYBRID universe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11570185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS (FULL MPD, FIXED): n_users=100000, n_items=514672\n",
      "Num interactions (nnz): 6,175,813\n",
      "item_user shape: (514672, 100000)\n",
      "Fitting ALS on item_user (items × users)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kiera\\Music_Recommender\\.venv\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS training done.\n",
      "ALS item_factors items: 100000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mALS item_factors items:\u001b[39m\u001b[33m\"\u001b[39m, n_items_als)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Sanity checks: no truncation now\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m n_items_als == n_items == \u001b[38;5;28mlen\u001b[39m(idx_to_tid_als)\n\u001b[32m     52\u001b[39m tid_to_item_idx_als = {tid: i \u001b[38;5;28;01mfor\u001b[39;00m i, tid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx_to_tid_als)}\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExample ALS item mapping:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(tid_to_item_idx_als.items())[:\u001b[32m3\u001b[39m])\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 06 - Build ALS user–item matrix on FULL MPD interactions (correct orientation)\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import implicit\n",
    "\n",
    "als_inter = inter_df.copy()  # use FULL MPD inter_df, not the restricted one\n",
    "\n",
    "# Encode playlists and tracks as categorical indices\n",
    "pid_codes = als_inter[\"pid\"].astype(\"category\")       # playlists = users\n",
    "tid_codes = als_inter[\"track_id\"].astype(\"category\")  # tracks = items\n",
    "\n",
    "als_inter[\"pid_idx\"] = pid_codes.cat.codes\n",
    "als_inter[\"tid_idx\"] = tid_codes.cat.codes\n",
    "\n",
    "n_users = als_inter[\"pid_idx\"].nunique()\n",
    "n_items = als_inter[\"tid_idx\"].nunique()\n",
    "\n",
    "print(f\"ALS (FULL MPD, CORRECT): n_users={n_users}, n_items={n_items}\")\n",
    "print(f\"Num interactions (nnz): {len(als_inter):,}\")\n",
    "\n",
    "# Build **users × items** matrix (this is what implicit expects)\n",
    "alpha = 1.0  # confidence scaling\n",
    "data = np.ones(len(als_inter), dtype=np.float32) * alpha\n",
    "\n",
    "rows = als_inter[\"pid_idx\"].values   # users\n",
    "cols = als_inter[\"tid_idx\"].values   # items\n",
    "\n",
    "user_items = coo_matrix((data, (rows, cols)), shape=(n_users, n_items)).tocsr()\n",
    "print(\"user_items shape:\", user_items.shape)\n",
    "\n",
    "# Map ALS internal item index -> track_id\n",
    "idx_to_tid_als = np.array(tid_codes.cat.categories)\n",
    "\n",
    "als_model = implicit.als.AlternatingLeastSquares(\n",
    "    factors=64,\n",
    "    regularization=0.01,\n",
    "    iterations=10,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Fitting ALS on user_items (users × items)...\")\n",
    "als_model.fit(user_items)   # <-- NO .T now\n",
    "print(\"ALS training done.\")\n",
    "\n",
    "n_items_als = als_model.item_factors.shape[0]\n",
    "print(\"ALS item_factors items:\", n_items_als)\n",
    "\n",
    "# Sanity checks – these should now pass with 514,672\n",
    "assert n_items_als == n_items == len(idx_to_tid_als)\n",
    "\n",
    "# Build clean mapping: track_id -> ALS item index\n",
    "tid_to_item_idx_als = {tid: i for i, tid in enumerate(idx_to_tid_als)}\n",
    "print(\"Example ALS item mapping:\", list(tid_to_item_idx_als.items())[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07 - Cluster HYBRID-universe tracks in audio-feature space\n",
    "\n",
    "N_CLUSTERS = 20\n",
    "\n",
    "print(f\"Clustering tracks with KMeans (K={N_CLUSTERS})...\")\n",
    "kmeans_tracks = KMeans(\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    random_state=42,\n",
    "    n_init=\"auto\",\n",
    ")\n",
    "track_cluster_labels = kmeans_tracks.fit_predict(X_norm_u)\n",
    "print(\"Track clustering complete.\")\n",
    "\n",
    "# Store cluster labels in a DataFrame for convenience\n",
    "track_cluster_df = pd.DataFrame({\n",
    "    \"track_id\": track_ids_u,\n",
    "    \"cluster_id\": track_cluster_labels,\n",
    "}).set_index(\"track_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51213d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08 - Helper functions: describe_tracks, search_tracks_by_name, get_cluster_id\n",
    "\n",
    "def describe_tracks(track_ids, extra_cols=None, top_n=10):\n",
    "    \"\"\"\n",
    "    Return metadata (and optional audio features) for given track_ids.\n",
    "    Uses HYBRID-universe metadata and features.\n",
    "    \"\"\"\n",
    "    track_ids = list(track_ids)\n",
    "\n",
    "    # --- metadata ---\n",
    "    meta_sub = (\n",
    "        meta_df_u[[\"track_id\", \"track_name\", \"artist_name\", \"album_name\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"track_id\")\n",
    "        .loc[lambda df: df.index.intersection(track_ids)]\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- optional audio features ---\n",
    "    if extra_cols:\n",
    "        feat_idx = feat_df_u.set_index(\"track_id\")\n",
    "        common_ids = feat_idx.index.intersection(track_ids)\n",
    "        feat_sub = feat_idx.loc[common_ids, extra_cols].reset_index()\n",
    "        meta_sub = meta_sub.merge(feat_sub, on=\"track_id\", how=\"left\")\n",
    "\n",
    "    if top_n is not None:\n",
    "        meta_sub = meta_sub.head(top_n)\n",
    "\n",
    "    return meta_sub\n",
    "\n",
    "\n",
    "def search_tracks_by_name(query, max_results=10):\n",
    "    \"\"\"\n",
    "    Simple case-insensitive substring search over track_name in HYBRID universe.\n",
    "    \"\"\"\n",
    "    q = query.lower()\n",
    "    hits = (\n",
    "        meta_df_u\n",
    "        .loc[meta_df_u[\"track_name\"].str.lower().str.contains(q, na=False),\n",
    "             [\"track_id\", \"track_name\", \"artist_name\", \"album_name\"]]\n",
    "        .drop_duplicates()\n",
    "        .head(max_results)\n",
    "    )\n",
    "    return hits\n",
    "\n",
    "\n",
    "def get_cluster_id(track_id):\n",
    "    \"\"\"\n",
    "    Return cluster_id for a track_id, or None if not clustered.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(track_cluster_df.loc[track_id, \"cluster_id\"])\n",
    "    except KeyError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ef24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 - Hybrid config: neighbour counts & candidate pool\n",
    "\n",
    "# How many neighbours we ask from each individual model\n",
    "COS_TOPN   = 1000   # cosine neighbours in feature space\n",
    "ALS_TOPN   = 1000   # ALS similar items\n",
    "I2V_TOPN   = 1000   # item2vec neighbours\n",
    "\n",
    "# Cap on the final union of candidates before scoring\n",
    "HYBRID_CANDIDATE_POOL = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcca726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 - Single-model scorers: cosine, ALS, item2vec\n",
    "\n",
    "\n",
    "def cosine_scores_for_track(track_id, top_n=COS_TOPN):\n",
    "    \"\"\"\n",
    "    Get cosine-similar tracks in feature space for a given track_id.\n",
    "    Returns a DataFrame: [track_id, cosine_sim]\n",
    "    \"\"\"\n",
    "    if track_id not in tid_to_feat_idx_u:\n",
    "        return pd.DataFrame(columns=[\"track_id\", \"cosine_sim\"])\n",
    "    \n",
    "    idx = tid_to_feat_idx_u[track_id]\n",
    "    vec = X_norm_u[idx : idx + 1]\n",
    "\n",
    "    # +1 to include the seed; we'll drop it\n",
    "    distances, indices = knn_cosine_u.kneighbors(vec, n_neighbors=top_n + 1)\n",
    "    distances = distances[0]\n",
    "    indices = indices[0]\n",
    "\n",
    "    sims = 1.0 - distances  # cosine similarity\n",
    "\n",
    "    neigh_tids = track_ids_u[indices]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"track_id\": neigh_tids,\n",
    "        \"cosine_sim\": sims,\n",
    "    })\n",
    "    # drop self\n",
    "    df = df[df[\"track_id\"] != track_id].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def als_scores_for_track(track_id, top_n=ALS_TOPN):\n",
    "    \"\"\"\n",
    "    Get ALS similar-item scores for a given track_id.\n",
    "    Returns: [track_id, als_sim]\n",
    "    \"\"\"\n",
    "    if track_id not in tid_to_item_idx_als:\n",
    "        # This track has no ALS item factors\n",
    "        return pd.DataFrame(columns=[\"track_id\", \"als_sim\"])\n",
    "\n",
    "    item_idx = tid_to_item_idx_als[track_id]\n",
    "    n_items_als = als_model.item_factors.shape[0]\n",
    "    if not (0 <= item_idx < n_items_als):\n",
    "        # out of range for some reason -> treat as no ALS signal\n",
    "        return pd.DataFrame(columns=[\"track_id\", \"als_sim\"])\n",
    "\n",
    "    sim_items, sim_scores = als_model.similar_items(\n",
    "        itemid=item_idx,\n",
    "        N=top_n + 1,   # include self\n",
    "    )\n",
    "\n",
    "    neigh_tids = idx_to_tid_als[sim_items]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"track_id\": neigh_tids,\n",
    "        \"als_sim\": sim_scores,\n",
    "    })\n",
    "\n",
    "    # Drop the seed track itself\n",
    "    df = df[df[\"track_id\"] != track_id].reset_index(drop=True)\n",
    "\n",
    "    return df.head(top_n)\n",
    "\n",
    "\n",
    "def item2vec_scores_for_track(track_id, top_n=I2V_TOPN):\n",
    "    \"\"\"\n",
    "    Get item2vec neighbours for a given track_id.\n",
    "    Returns: [track_id, item2vec_sim]\n",
    "    \"\"\"\n",
    "    if track_id not in item2vec_kv:\n",
    "        return pd.DataFrame(columns=[\"track_id\", \"item2vec_sim\"])\n",
    "    \n",
    "    most_sim = item2vec_kv.most_similar(track_id, topn=top_n + 1)\n",
    "\n",
    "    neigh_tids = []\n",
    "    neigh_scores = []\n",
    "    for tid, score in most_sim:\n",
    "        if tid == track_id:\n",
    "            continue\n",
    "        neigh_tids.append(tid)\n",
    "        neigh_scores.append(score)\n",
    "        if len(neigh_tids) >= top_n:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"track_id\": neigh_tids,\n",
    "        \"item2vec_sim\": neigh_scores,\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 - Hybrid scores: union of candidates + recompute full scores for each model\n",
    "\n",
    "\n",
    "def hybrid_scores_for_track(\n",
    "    track_id,\n",
    "    w_cos=0.3,\n",
    "    w_als=0.3,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.05,        # smaller default cluster weight\n",
    "    top_k=30,\n",
    "    candidate_pool=HYBRID_CANDIDATE_POOL,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute hybrid scores for neighbours of a single seed track.\n",
    "\n",
    "    KEY CHANGE vs previous version:\n",
    "    -------------------------------\n",
    "    - We still build a candidate set from the union of:\n",
    "        * cosine neighbours\n",
    "        * ALS similar-items\n",
    "        * item2vec neighbours\n",
    "      (this gives a diverse set of plausible candidates)\n",
    "    - BUT then we RECOMPUTE all three similarities for EVERY candidate\n",
    "      wherever possible:\n",
    "\n",
    "        cosine_sim   -> always defined for tracks with features (HYBRID universe)\n",
    "        als_sim      -> defined if both seed & candidate have ALS factors\n",
    "        item2vec_sim -> defined if both seed & candidate are in item2vec vocab\n",
    "\n",
    "      So NaNs now mean \"this model truly has no signal for this track\",\n",
    "      not \"we didn't happen to fetch it in the top-N list\".\n",
    "\n",
    "    Returns a DataFrame with:\n",
    "      [track_id, cosine_sim, als_sim, item2vec_sim, same_cluster, hybrid_score]\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 1. Build initial candidate set from all three models ----------\n",
    "\n",
    "    df_cos = cosine_scores_for_track(track_id, top_n=COS_TOPN)\n",
    "    df_als = als_scores_for_track(track_id, top_n=ALS_TOPN)\n",
    "    df_i2v = item2vec_scores_for_track(track_id, top_n=I2V_TOPN)\n",
    "\n",
    "    # Outer merge = union of all candidates\n",
    "    df = (\n",
    "        df_cos\n",
    "        .merge(df_als, on=\"track_id\", how=\"outer\")\n",
    "        .merge(df_i2v, on=\"track_id\", how=\"outer\")\n",
    "    )\n",
    "\n",
    "    # Drop seed if it appears\n",
    "    df = df[df[\"track_id\"] != track_id].reset_index(drop=True)\n",
    "\n",
    "    if df.empty:\n",
    "        if verbose:\n",
    "            print(f\"No candidates from any model for seed {track_id}\")\n",
    "        return df.assign(\n",
    "            same_cluster=pd.Series(dtype=float),\n",
    "            hybrid_score=pd.Series(dtype=float),\n",
    "        )\n",
    "\n",
    "    # If too many candidates, keep those with the largest max raw score\n",
    "    if len(df) > candidate_pool:\n",
    "        df[\"max_raw_score\"] = df[[\"cosine_sim\", \"als_sim\", \"item2vec_sim\"]].max(axis=1, skipna=True)\n",
    "        df = df.sort_values(\"max_raw_score\", ascending=False).head(candidate_pool).reset_index(drop=True)\n",
    "        df = df.drop(columns=[\"max_raw_score\"])\n",
    "\n",
    "    # We now have a candidate list. We'll overwrite cosine_sim / als_sim / item2vec_sim\n",
    "    candidate_ids = df[\"track_id\"].tolist()\n",
    "\n",
    "    # ---------- 2. Recompute cosine_sim for ALL candidates (where possible) ----------\n",
    "\n",
    "    cos_vals = []\n",
    "    if track_id in tid_to_feat_idx_u:\n",
    "        seed_idx = tid_to_feat_idx_u[track_id]\n",
    "        seed_vec = X_norm_u[seed_idx]\n",
    "        for tid in candidate_ids:\n",
    "            idx = tid_to_feat_idx_u.get(tid)\n",
    "            if idx is None:\n",
    "                cos_vals.append(np.nan)\n",
    "            else:\n",
    "                cand_vec = X_norm_u[idx]\n",
    "                # dot of L2-normalised vectors = cosine similarity\n",
    "                cos_vals.append(float(np.dot(seed_vec, cand_vec)))\n",
    "    else:\n",
    "        # seed not in feature space (shouldn't happen in HYBRID universe),\n",
    "        # but handle gracefully\n",
    "        cos_vals = [np.nan] * len(candidate_ids)\n",
    "\n",
    "    df[\"cosine_sim\"] = pd.Series(cos_vals, index=df.index, dtype=\"float64\")\n",
    "\n",
    "    # ---------- 3. Recompute ALS similarity for ALL candidates (where possible) ----------\n",
    "\n",
    "    als_vals = []\n",
    "    if track_id in tid_to_item_idx_als:\n",
    "        seed_item_idx = tid_to_item_idx_als[track_id]\n",
    "        seed_vec = als_model.item_factors[seed_item_idx]\n",
    "        for tid in candidate_ids:\n",
    "            cand_idx = tid_to_item_idx_als.get(tid)\n",
    "            if cand_idx is None:\n",
    "                als_vals.append(np.nan)\n",
    "            else:\n",
    "                cand_vec = als_model.item_factors[cand_idx]\n",
    "                # Simple dot product in latent space\n",
    "                als_vals.append(float(np.dot(seed_vec, cand_vec)))\n",
    "    else:\n",
    "        als_vals = [np.nan] * len(candidate_ids)\n",
    "\n",
    "    df[\"als_sim\"] = pd.Series(als_vals, index=df.index, dtype=\"float64\")\n",
    "\n",
    "    # ---------- 4. Recompute item2vec similarity for ALL candidates (where possible) ----------\n",
    "\n",
    "    i2v_vals = []\n",
    "    has_seed_i2v = track_id in item2vec_kv.key_to_index\n",
    "    for tid in candidate_ids:\n",
    "        if not has_seed_i2v or tid not in item2vec_kv.key_to_index:\n",
    "            i2v_vals.append(np.nan)\n",
    "        else:\n",
    "            # gensim KeyedVectors already stores normalised vectors, so this is cosine\n",
    "            i2v_vals.append(float(item2vec_kv.similarity(track_id, tid)))\n",
    "\n",
    "    df[\"item2vec_sim\"] = pd.Series(i2v_vals, index=df.index, dtype=\"float64\")\n",
    "\n",
    "    # ---------- 5. Coverage diagnostics (AFTER recompute) ----------\n",
    "\n",
    "    if verbose:\n",
    "        n_cand = len(df)\n",
    "        cov_cos = df[\"cosine_sim\"].notna().mean() if n_cand else 0.0\n",
    "        cov_als = df[\"als_sim\"].notna().mean() if n_cand else 0.0\n",
    "        cov_i2v = df[\"item2vec_sim\"].notna().mean() if n_cand else 0.0\n",
    "        print(\n",
    "            f\"[COVERAGE] seed={track_id}, candidates={n_cand}, \"\n",
    "            f\"cosine={cov_cos:.1%}, ALS={cov_als:.1%}, item2vec={cov_i2v:.1%}\"\n",
    "        )\n",
    "\n",
    "    # ---------- 6. Same-cluster flag ----------\n",
    "\n",
    "    seed_cluster = get_cluster_id(track_id)\n",
    "    df[\"same_cluster\"] = df[\"track_id\"].apply(\n",
    "        lambda tid: 1.0 if (seed_cluster is not None and get_cluster_id(tid) == seed_cluster) else 0.0\n",
    "    )\n",
    "\n",
    "    # ---------- 7. Min–max normalise each model's scores over candidates ----------\n",
    "\n",
    "    def col_minmax_numeric(series):\n",
    "        # ensure numeric float dtype to avoid FutureWarning\n",
    "        series = pd.to_numeric(series, errors=\"coerce\")\n",
    "        if series.notna().sum() == 0:\n",
    "            return series.astype(\"float64\")\n",
    "        cmin = series.min(skipna=True)\n",
    "        cmax = series.max(skipna=True)\n",
    "        if cmax == cmin:\n",
    "            # all same -> treat as 1.0 where present\n",
    "            return series.notna().astype(float)\n",
    "        return (series - cmin) / (cmax - cmin)\n",
    "\n",
    "    df[\"cosine_norm\"]   = col_minmax_numeric(df[\"cosine_sim\"])\n",
    "    df[\"als_norm\"]      = col_minmax_numeric(df[\"als_sim\"])\n",
    "    df[\"item2vec_norm\"] = col_minmax_numeric(df[\"item2vec_sim\"])\n",
    "\n",
    "    # ---------- 8. Per-track weight renormalisation (only models that actually have signal) ----------\n",
    "\n",
    "    cos_present = df[\"cosine_norm\"].notna().astype(float)\n",
    "    als_present = df[\"als_norm\"].notna().astype(float)\n",
    "    i2v_present = df[\"item2vec_norm\"].notna().astype(float)\n",
    "\n",
    "    eff_cos_w = w_cos * cos_present\n",
    "    eff_als_w = w_als * als_present\n",
    "    eff_i2v_w = w_i2v * i2v_present\n",
    "\n",
    "    weight_sum = eff_cos_w + eff_als_w + eff_i2v_w\n",
    "\n",
    "    valid_mask = weight_sum > 0\n",
    "    df = df[valid_mask].copy()\n",
    "    eff_cos_w = eff_cos_w[valid_mask]\n",
    "    eff_als_w = eff_als_w[valid_mask]\n",
    "    eff_i2v_w = eff_i2v_w[valid_mask]\n",
    "    weight_sum = weight_sum[valid_mask]\n",
    "\n",
    "    # Normalised weights per track\n",
    "    w_cos_norm = eff_cos_w / weight_sum\n",
    "    w_als_norm = eff_als_w / weight_sum\n",
    "    w_i2v_norm = eff_i2v_w / weight_sum\n",
    "\n",
    "    # Replace NaNs with 0.0 for multiplication\n",
    "    df[\"cosine_norm\"]   = pd.to_numeric(df[\"cosine_norm\"], errors=\"coerce\").fillna(0.0)\n",
    "    df[\"als_norm\"]      = pd.to_numeric(df[\"als_norm\"], errors=\"coerce\").fillna(0.0)\n",
    "    df[\"item2vec_norm\"] = pd.to_numeric(df[\"item2vec_norm\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    base_hybrid = (\n",
    "        w_cos_norm.to_numpy() * df[\"cosine_norm\"].to_numpy()\n",
    "        + w_als_norm.to_numpy() * df[\"als_norm\"].to_numpy()\n",
    "        + w_i2v_norm.to_numpy() * df[\"item2vec_norm\"].to_numpy()\n",
    "    )\n",
    "\n",
    "    # Cluster bonus as a small bump, not a sledgehammer\n",
    "    df[\"hybrid_score\"] = base_hybrid + w_cluster * df[\"same_cluster\"].to_numpy()\n",
    "\n",
    "    # ---------- 9. Final tidy-up ----------\n",
    "\n",
    "    keep_cols = [\n",
    "        \"track_id\",\n",
    "        \"cosine_sim\", \"als_sim\", \"item2vec_sim\",\n",
    "        \"same_cluster\",\n",
    "        \"hybrid_score\",\n",
    "    ]\n",
    "    df = df[keep_cols].sort_values(\"hybrid_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df.head(top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 - User-facing helper: recommend_by_name_hybrid + demo examples\n",
    "\n",
    "\n",
    "def recommend_by_name_hybrid(\n",
    "    query,\n",
    "    candidate_index=0,\n",
    "    w_cos=0.3,\n",
    "    w_als=0.3,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.1,\n",
    "    top_k=20,\n",
    "    candidate_pool=HYBRID_CANDIDATE_POOL,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience wrapper:\n",
    "      - fuzzy search by track_name within HYBRID universe\n",
    "      - pick one candidate by index\n",
    "      - compute hybrid recommendations\n",
    "      - attach metadata + audio features\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. search ---\n",
    "    hits = search_tracks_by_name(query, max_results=10)\n",
    "    if hits.empty:\n",
    "        print(f\"No matches for query '{query}'\")\n",
    "        return None, pd.DataFrame()\n",
    "\n",
    "    print(\"Search results:\")\n",
    "    display(hits.reset_index(drop=True))\n",
    "\n",
    "    if candidate_index >= len(hits):\n",
    "        raise ValueError(f\"candidate_index {candidate_index} out of range for {len(hits)} results\")\n",
    "\n",
    "    seed_row = hits.iloc[candidate_index]\n",
    "    seed_tid = seed_row[\"track_id\"]\n",
    "\n",
    "    print(\"\\nChosen seed track:\")\n",
    "    display(seed_row.to_frame().T)\n",
    "\n",
    "    # --- 2. hybrid scores ---\n",
    "    df_scores = hybrid_scores_for_track(\n",
    "        seed_tid,\n",
    "        w_cos=w_cos,\n",
    "        w_als=w_als,\n",
    "        w_i2v=w_i2v,\n",
    "        w_cluster=w_cluster,\n",
    "        top_k=top_k,\n",
    "        candidate_pool=candidate_pool,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    if df_scores.empty:\n",
    "        print(\"No hybrid candidates found.\")\n",
    "        return seed_row, df_scores\n",
    "\n",
    "    # --- 3. attach metadata + a few audio features ---\n",
    "    rec_df = describe_tracks(\n",
    "        df_scores[\"track_id\"].tolist(),\n",
    "        extra_cols=[\"danceability\", \"energy\", \"valence\", \"tempo\"],\n",
    "        top_n=None,\n",
    "    )\n",
    "\n",
    "    rec_df = rec_df.merge(df_scores, on=\"track_id\", how=\"left\")\n",
    "    rec_df = rec_df.sort_values(\"hybrid_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nHybrid recommendations:\")\n",
    "    display(rec_df.head(top_k))\n",
    "\n",
    "    return seed_row, rec_df\n",
    "\n",
    "\n",
    "# --- Demo examples for 5 seed songs ---\n",
    "\n",
    "# Example 1: Pitbull \"Hotel Room Service\"\n",
    "seed1, recs1 = recommend_by_name_hybrid(\n",
    "    \"hotel room service\",\n",
    "    candidate_index=0,\n",
    "    w_cos=0.2,\n",
    "    w_als=0.4,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.1,\n",
    "    top_k=15,\n",
    ")\n",
    "\n",
    "# Example 2: Billie Eilish \"ocean eyes\"\n",
    "seed2, recs2 = recommend_by_name_hybrid(\n",
    "    \"ocean eyes\",\n",
    "    candidate_index=1,  # original track on dont smile at me (adjust if needed)\n",
    "    w_cos=0.2,\n",
    "    w_als=0.4,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.1,\n",
    "    top_k=15,\n",
    ")\n",
    "\n",
    "# Example 3: Travis Scott \"90210\"\n",
    "seed3, recs3 = recommend_by_name_hybrid(\n",
    "    \"90210\",\n",
    "    candidate_index=5,  # Travis Scott \"90210\" (adjust if search order changes)\n",
    "    w_cos=0.25,\n",
    "    w_als=0.35,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.1,\n",
    "    top_k=15,\n",
    ")\n",
    "\n",
    "# Example 4: Beatles \"Yesterday - Remastered\"\n",
    "seed4, recs4 = recommend_by_name_hybrid(\n",
    "    \"yesterday - remastered\",\n",
    "    candidate_index=0,\n",
    "    w_cos=0.25,\n",
    "    w_als=0.35,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.1,\n",
    "    top_k=15,\n",
    ")\n",
    "\n",
    "# Example 5: Kanye West \"Gold Digger\"\n",
    "seed5, recs5 = recommend_by_name_hybrid(\n",
    "    \"gold digger\",\n",
    "    candidate_index=3,  # Kanye \"Gold Digger\" (adjust if search order changes)\n",
    "    w_cos=0.2,\n",
    "    w_als=0.4,\n",
    "    w_i2v=0.3,\n",
    "    w_cluster=0.1,\n",
    "    top_k=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedad083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 - PCA on a sample of tracks for 2D visualisation\n",
    "\n",
    "N_SAMPLE = 20000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "idx_sample = rng.choice(X_norm_u.shape[0], size=min(N_SAMPLE, X_norm_u.shape[0]), replace=False)\n",
    "X_sample = X_norm_u[idx_sample]\n",
    "tids_sample = track_ids_u[idx_sample]\n",
    "clusters_sample = track_cluster_labels[idx_sample]\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca_sample = pca.fit_transform(X_sample)\n",
    "\n",
    "pca_df = pd.DataFrame({\n",
    "    \"x\": X_pca_sample[:, 0],\n",
    "    \"y\": X_pca_sample[:, 1],\n",
    "    \"track_id\": tids_sample,\n",
    "    \"cluster_id\": clusters_sample,\n",
    "}).set_index(\"track_id\")\n",
    "\n",
    "print(\"PCA sample shape:\", pca_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c494a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 - Visual helper: plot seed + hybrid recommendations in PCA space\n",
    "\n",
    "def plot_seed_and_recs_on_pca(seed_row, rec_df, title_suffix=\"\"):\n",
    "    seed_tid = seed_row[\"track_id\"]\n",
    "\n",
    "    rec_ids = rec_df[\"track_id\"].tolist()\n",
    "    all_ids = [seed_tid] + rec_ids\n",
    "\n",
    "    df = pca_df.loc[pca_df.index.intersection(all_ids)].copy()\n",
    "    if df.empty:\n",
    "        print(\"Seed + recs not found in PCA sample (downsampling issue).\")\n",
    "        return\n",
    "\n",
    "    df[\"is_seed\"] = (df.index == seed_tid)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "\n",
    "    # Background: all sampled tracks (faint)\n",
    "    plt.scatter(\n",
    "        pca_df[\"x\"],\n",
    "        pca_df[\"y\"],\n",
    "        s=4,\n",
    "        alpha=0.1,\n",
    "        label=\"Other tracks\",\n",
    "    )\n",
    "\n",
    "    # Recommendations\n",
    "    rec_mask = ~df[\"is_seed\"]\n",
    "    plt.scatter(\n",
    "        df.loc[rec_mask, \"x\"],\n",
    "        df.loc[rec_mask, \"y\"],\n",
    "        s=40,\n",
    "        alpha=0.9,\n",
    "        label=\"Hybrid recs\",\n",
    "    )\n",
    "\n",
    "    # Seed\n",
    "    plt.scatter(\n",
    "        df.loc[df[\"is_seed\"], \"x\"],\n",
    "        df.loc[df[\"is_seed\"], \"y\"],\n",
    "        s=120,\n",
    "        marker=\"*\",\n",
    "        label=\"Seed track\",\n",
    "    )\n",
    "\n",
    "    plt.title(f\"PCA of audio features – seed & hybrid recs\\n{title_suffix}\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example visualisations\n",
    "if seed1 is not None and not recs1.empty:\n",
    "    plot_seed_and_recs_on_pca(seed1, recs1, title_suffix=seed1[\"track_name\"])\n",
    "if seed2 is not None and not recs2.empty:\n",
    "    plot_seed_and_recs_on_pca(seed2, recs2, title_suffix=seed2[\"track_name\"])\n",
    "if seed5 is not None and not recs5.empty:\n",
    "    plot_seed_and_recs_on_pca(seed5, recs5, title_suffix=seed5[\"track_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tid, name in [\n",
    "    (\"0OPyDgTRuIdCJ9B4bYSths\", \"Hotel Room Service\"),\n",
    "    (\"3OMh7VdOoWgtKhJimQQywz\", \"ocean eyes\"),\n",
    "    (\"51EC3I1nQXpec4gDk0mQyP\", \"90210 (Travis Scott)\"),\n",
    "    (\"5XJJdNPkwmbUwE79gv0NxK\", \"Gold Digger\"),\n",
    "    (\"1e0hllQ23AG0QGFgezgLOq\", \"Yesterday\"),\n",
    "]:\n",
    "    in_als = tid in tid_to_item_idx_als  # or the current ALS mapping name\n",
    "    n_playlists = inter_df_u.loc[inter_df_u[\"track_id\"] == tid, \"pid\"].nunique()\n",
    "    print(name, tid, \"in_als?\", in_als, \"playlists:\", n_playlists)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
